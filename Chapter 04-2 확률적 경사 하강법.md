## 핵심 키워드
> **점진적 학습**: 훈련 데이터에 새로운 데이터가 추가되었을 때, 모델을 처음부터 다시 학습하지 않고
> 새로운 데이터에 대해서 조금씩 훈련하는 훈련 방식

> **손실 함수(loss function)**: 머신러닝/딥러닝 모델이 예측한 값과 실제 정답 값 사이의 '차이' 또는 '오차'를 수치로 측정하는 함수. 손실 함수의 값이 작을수록 모델의 성능이 뛰어나다.
> 분류 모델에서 대표적으로 쓰이는 손실 함수로는 logistic(binary cross-entropy), softmax, hinge, categorical cross-entrophy가 있다. 회귀 모델에선 mean squared error 함수가 널리 쓰인다.

## 확률적 경사 하강법(stochastic gradient descent, SGD)**
> 경사 하강법이란, 손실 함수의 기울기를 따라 내려가며 파라미터를 조정해 손실 함수의 값을 최대한 작게 최적화하며 모델의 성능을 높이는 알고리즘이다.  
> 이때 데이터를 무작위로 하나 뽑아서 쓰면 확률적, 여러개를 뽑아서 쓰면 mini batch, 모든 데이터를 한 번에 쓰면 batch 경사 하강법이다.
> SGD 방식으로 훈련할 때, 훈련 세트를 한 번 모두 사용하는 과정을 **에포크(epoch)**라고 한다. 보통 SGD는 수천, 수만 번 에포크를 반복하며 학습한다.

> 손실 함수의 경사를 하강한다는 것은, 손실 함수가 모델의 파라미터에 대해 미분 가능할 때,  
> 현재 파라미터 θ에서 손실 함수 L(θ)의 기울기(gradient) ∇θL을 계산하고,  
> 그 기울기의 반대 방향(−∇θL)으로 파라미터를 조금 이동시키는 업데이트를 반복함으로써,  
> 손실 함수가 낮아지는 방향으로 파라미터를 점진적으로 조정하여  
> 손실 함수의 지역 최소점(혹은 전역 최소점)에 도달하는 방법을 말한다.  
